{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation\n",
    "\n",
    "Train and evaluate a series of KMeans models to find the best performing model by choosing a value for **k**.\n",
    "\n",
    "## Steps:\n",
    "\n",
    "1. **Load the Clean, Combined Dataset**\n",
    "   - Load the preprocessed and combined dataset containing the audio features.\n",
    "\n",
    "2. **Select Audio Features Based on Description**\n",
    "   - Choose the relevant audio features from the dataset for clustering.\n",
    "\n",
    "3. **Scale the Dataset**\n",
    "   - Apply scaling (e.g., StandardScaler) to normalize the features before training the model.\n",
    "\n",
    "4. **Train a Range of Models with Different k Values**\n",
    "   - Train multiple KMeans models using different values for **k** (e.g., k=2, 3, 4, ..., 10).\n",
    "\n",
    "5. **Evaluate and Select the Top 2 Values for k**\n",
    "   - Use the **Elbow Method** to visually inspect the optimal number of clusters.\n",
    "   - Use the **Silhouette Score** to evaluate how well-defined the clusters are.\n",
    "   \n",
    "6. **Try a Live Test with the Selected Models**\n",
    "   - Test the two top-performing models (based on the Elbow Method and Silhouette Score) in a live setting.\n",
    "   - Select the best performing value of **k** based on the test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/clean/spotify_data_encoded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_basic_info(df):\n",
    "    \"\"\"\n",
    "    Display basic information about the dataset including shape, data types, and missing values\n",
    "    \"\"\"\n",
    "    print('Dataset Shape:', df.shape)\n",
    "    print('\\nData Types:')\n",
    "    print(df.dtypes)\n",
    "    print('\\nMissing Values:')\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "def display_numerical_summary(df):\n",
    "    \"\"\"\n",
    "    Display summary statistics for numerical columns\n",
    "    \"\"\"\n",
    "    print('Numerical Columns Summary:')\n",
    "    print(df.describe())\n",
    "\n",
    "def check_duplicates(df):\n",
    "    \"\"\"\n",
    "    Check for duplicate entries in the dataset\n",
    "    \"\"\"\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f'Number of duplicate entries: {duplicates}')\n",
    "    \n",
    "def display_unique_values(df, columns):\n",
    "    \"\"\"\n",
    "    Display number of unique values for specified columns, with special handling for the genres column\n",
    "    \"\"\"\n",
    "    print('Unique Values Count:')\n",
    "    for col in columns:\n",
    "        if col == 'genres':\n",
    "            all_genres = []\n",
    "            for genre_list in df[col].dropna():\n",
    "                if isinstance(genre_list, str):\n",
    "                    genre_list = eval(genre_list)\n",
    "                all_genres.extend(genre_list)\n",
    "            unique_genres = len(set(all_genres))\n",
    "            print(f'{col}: {unique_genres} unique genres')\n",
    "        else:\n",
    "            print(f'{col}: {df[col].nunique()} unique values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_basic_info(df)\n",
    "\n",
    "check_duplicates(df)\n",
    "\n",
    "display_unique_values(df, ['artist', 'release_year', 'genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant audio features (music genres)\n",
    "features = data[['rock', 'pop', 'blues', 'metal', 'hip-hop', 'country', \n",
    "                 'punk', 'jazz', 'rap', 'reggae', 'folk', 'soul', 'latin', \n",
    "                 'dance', 'indie', 'classical']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale the features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Models with Different k Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant features (encoded genres)\n",
    "features = df[[\n",
    "    'rock', 'pop', 'blues', 'metal', 'hip-hop', 'country', \n",
    "    'punk', 'jazz', 'rap', 'reggae', 'folk', 'soul', 'latin', \n",
    "    'dance', 'indie', 'classical'\n",
    "]]\n",
    "\n",
    "# scale the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# train models with different k values\n",
    "k_values = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "models = [KMeans(n_clusters=k, random_state=42) for k in k_values]\n",
    "\n",
    "# inertias for elbow method\n",
    "inertias = []\n",
    "\n",
    "for model in models:\n",
    "    model.fit(scaled_features)\n",
    "    inertias.append(model.inertia_)\n",
    "    print(f'Model trained with k={model.n_clusters}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Models and Collect Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = range(2, 147)\n",
    "results = []\n",
    "\n",
    "for k in k_values:\n",
    "    # train model\n",
    "    model = KMeans(n_clusters=k, random_state=42)\n",
    "    model.fit(scaled_features)\n",
    "    \n",
    "    # metrics\n",
    "    inertia = model.inertia_\n",
    "    silhouette = silhouette_score(scaled_features, model.labels_)\n",
    "    \n",
    "    results.append({\n",
    "        'k': k,\n",
    "        'inertia': inertia,\n",
    "        'silhouette': silhouette\n",
    "    })\n",
    "    print(f'k={k}: inertia={inertia:.0f}, silhouette={silhouette:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate using Elbow Method and Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Elbow Method Plot\n",
    "inertias = [r['inertia'] for r in results]\n",
    "ax1.plot(k_values, inertias, 'bo-')\n",
    "ax1.set_xlabel('k')\n",
    "ax1.set_ylabel('Inertia')\n",
    "ax1.set_title('Elbow Method')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Silhouette Score Plot\n",
    "silhouettes = [r['silhouette'] for r in results]\n",
    "ax2.plot(k_values, silhouettes, 'ro-')\n",
    "ax2.set_xlabel('k')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.set_title('Silhouette Analysis')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find top 2 k values based on silhouette score\n",
    "top_k_values = sorted(results, key=lambda x: x['silhouette'], reverse=True)[:2]\n",
    "print(\"\\nTop 2 k values based on silhouette score:\")\n",
    "for result in top_k_values:\n",
    "    print(f\"k={result['k']}: silhouette={result['silhouette']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Train Final Models with Top k Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_models = {}\n",
    "for result in top_k_values:\n",
    "    k = result['k']\n",
    "    model = KMeans(n_clusters=k, random_state=42)\n",
    "    model.fit(scaled_features)\n",
    "    final_models[k] = model\n",
    "    \n",
    "    # Print cluster sizes\n",
    "    unique, counts = np.unique(model.labels_, return_counts=True)\n",
    "    print(f\"\\nCluster sizes for k={k}:\")\n",
    "    for cluster, size in zip(unique, counts):\n",
    "        print(f\"Cluster {cluster}: {size} songs\")\n",
    "    \n",
    "    # Print most common genres in each cluster\n",
    "    cluster_centers = pd.DataFrame(\n",
    "        model.cluster_centers_,\n",
    "        columns=features.columns\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nDominant genres in each cluster (k={k}):\")\n",
    "    for i in range(k):\n",
    "        top_genres = cluster_centers.iloc[i].nlargest(3)\n",
    "        print(f\"Cluster {i}: {', '.join([f'{g} ({v:.2f})' for g, v in top_genres.items()])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
