{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation\n",
    "\n",
    "Train and evaluate a series of KMeans models to find the best performing model by choosing a value for **k**.\n",
    "\n",
    "## Steps:\n",
    "\n",
    "1. **Load the Clean, Combined Dataset**\n",
    "   - Load the preprocessed and combined dataset containing the audio features.\n",
    "\n",
    "2. **Select Audio Features Based on Description**\n",
    "   - Choose the relevant audio features from the dataset for clustering.\n",
    "\n",
    "3. **Scale the Dataset**\n",
    "   - Apply scaling (e.g., StandardScaler) to normalize the features before training the model.\n",
    "\n",
    "4. **Train a Range of Models with Different k Values**\n",
    "   - Train multiple KMeans models using different values for **k** (e.g., k=2, 3, 4, ..., 10).\n",
    "\n",
    "5. **Evaluate and Select the Top 2 Values for k**\n",
    "   - Use the **Elbow Method** to visually inspect the optimal number of clusters.\n",
    "   - Use the **Silhouette Score** to evaluate how well-defined the clusters are.\n",
    "   \n",
    "6. **Try a Live Test with the Selected Models**\n",
    "   - Test the two top-performing models (based on the Elbow Method and Silhouette Score) in a live setting.\n",
    "   - Select the best performing value of **k** based on the test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import pickle\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/clean/spotify_data_encoded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_basic_info(df):\n",
    "    \"\"\"\n",
    "    Display basic information about the dataset including shape, data types, and missing values\n",
    "    \"\"\"\n",
    "    print('Dataset Shape:', df.shape)\n",
    "    print('\\nData Types:')\n",
    "    print(df.dtypes)\n",
    "    print('\\nMissing Values:')\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "def display_numerical_summary(df):\n",
    "    \"\"\"\n",
    "    Display summary statistics for numerical columns\n",
    "    \"\"\"\n",
    "    print('Numerical Columns Summary:')\n",
    "    print(df.describe())\n",
    "\n",
    "def check_duplicates(df):\n",
    "    \"\"\"\n",
    "    Check for duplicate entries in the dataset\n",
    "    \"\"\"\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f'Number of duplicate entries: {duplicates}')\n",
    "    \n",
    "def display_unique_values(df, columns):\n",
    "    \"\"\"\n",
    "    Display number of unique values for specified columns, with special handling for the genres column\n",
    "    \"\"\"\n",
    "    print('Unique Values Count:')\n",
    "    for col in columns:\n",
    "        if col == 'genres':\n",
    "            all_genres = []\n",
    "            for genre_list in df[col].dropna():\n",
    "                if isinstance(genre_list, str):\n",
    "                    genre_list = eval(genre_list)\n",
    "                all_genres.extend(genre_list)\n",
    "            unique_genres = len(set(all_genres))\n",
    "            print(f'{col}: {unique_genres} unique genres')\n",
    "        else:\n",
    "            print(f'{col}: {df[col].nunique()} unique values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_basic_info(df)\n",
    "\n",
    "check_duplicates(df)\n",
    "\n",
    "display_unique_values(df, ['artist', 'release_year', 'genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant audio features (music genres)\n",
    "features = data[['rock', 'pop', 'blues', 'metal', 'hip-hop', 'country', \n",
    "                 'punk', 'jazz', 'rap', 'reggae', 'folk', 'soul', 'latin', \n",
    "                 'dance', 'indie', 'classical']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale the features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the scaler\n",
    "with open('../models/genre_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# tTest loading the scaler\n",
    "with open('../models/genre_scaler.pkl', 'rb') as f:\n",
    "    loaded_scaler = pickle.load(f)\n",
    "    \n",
    "# check if it works\n",
    "test_scaled = loaded_scaler.transform(features.head(1))\n",
    "print('Verification - scaled features shape:', test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Models with Different k Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant features (encoded genres)\n",
    "features = df[[\n",
    "    'rock', 'pop', 'blues', 'metal', 'hip-hop', 'country', \n",
    "    'punk', 'jazz', 'rap', 'reggae', 'folk', 'soul', 'latin', \n",
    "    'dance', 'indie', 'classical'\n",
    "]]\n",
    "\n",
    "# scale the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# train models with different k values\n",
    "k_values = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "models = [KMeans(n_clusters=k, random_state=42) for k in k_values]\n",
    "\n",
    "# inertias for elbow method\n",
    "inertias = []\n",
    "\n",
    "for model in models:\n",
    "    model.fit(scaled_features)\n",
    "    inertias.append(model.inertia_)\n",
    "    print(f'Model trained with k={model.n_clusters}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Models and Collect Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = range(2, 147)\n",
    "results = []\n",
    "\n",
    "for k in k_values:\n",
    "    # train model\n",
    "    model = KMeans(n_clusters=k, random_state=42)\n",
    "    model.fit(scaled_features)\n",
    "    \n",
    "    # metrics\n",
    "    inertia = model.inertia_\n",
    "    silhouette = silhouette_score(scaled_features, model.labels_)\n",
    "    \n",
    "    results.append({\n",
    "        'k': k,\n",
    "        'inertia': inertia,\n",
    "        'silhouette': silhouette\n",
    "    })\n",
    "    print(f'k={k}: inertia={inertia:.0f}, silhouette={silhouette:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate using Elbow Method and Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Elbow Method Plot\n",
    "inertias = [r['inertia'] for r in results]\n",
    "ax1.plot(k_values, inertias, 'bo-')\n",
    "ax1.set_xlabel('k')\n",
    "ax1.set_ylabel('Inertia')\n",
    "ax1.set_title('Elbow Method')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Silhouette Score Plot\n",
    "silhouettes = [r['silhouette'] for r in results]\n",
    "ax2.plot(k_values, silhouettes, 'ro-')\n",
    "ax2.set_xlabel('k')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.set_title('Silhouette Analysis')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find top 2 k values based on silhouette score\n",
    "top_k_values = sorted(results, key=lambda x: x['silhouette'], reverse=True)[:2]\n",
    "print('\\nTop 2 k values based on silhouette score:')\n",
    "for result in top_k_values:\n",
    "    print(f\"k={result['k']}: silhouette={result['silhouette']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Train Final Models with Top k Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_models = {}\n",
    "for result in top_k_values:\n",
    "    k = result['k']\n",
    "    model = KMeans(n_clusters=k, random_state=42)\n",
    "    model.fit(scaled_features)\n",
    "    final_models[k] = model\n",
    "    \n",
    "    #cluster sizes\n",
    "    unique, counts = np.unique(model.labels_, return_counts=True)\n",
    "    print(f'\\nCluster sizes for k={k}:')\n",
    "    for cluster, size in zip(unique, counts):\n",
    "        print(f'Cluster {cluster}: {size} songs')\n",
    "    \n",
    "    # most common genres per cluster\n",
    "    cluster_centers = pd.DataFrame(\n",
    "        model.cluster_centers_,\n",
    "        columns=features.columns\n",
    "    )\n",
    "    \n",
    "    print(f'\\nDominant genres in each cluster (k={k}):')\n",
    "    for i in range(k):\n",
    "        top_genres = cluster_centers.iloc[i].nlargest(3)\n",
    "        print(f\"Cluster {i}: {', '.join([f'{g} ({v:.2f})' for g, v in top_genres.items()])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clustering(scaled_features, model, feature_names):\n",
    "    \"\"\"\n",
    "    Evaluate clustering results using multiple metrics and visualizations\n",
    "    \"\"\"\n",
    "    labels = model.labels_\n",
    "    centers = model.cluster_centers_\n",
    "    \n",
    "    # basic metrics\n",
    "    silhouette = silhouette_score(scaled_features, labels)\n",
    "    inertia = model.inertia_\n",
    "    \n",
    "    print(f'\\nClustering Evaluation Metrics:')\n",
    "    print(f'Silhouette Score: {silhouette:.3f}')\n",
    "    print(f'Inertia: {inertia:.0f}')\n",
    "    \n",
    "    # cluster sizes\n",
    "    # unique, counts = np.unique(labels, return_counts=True)\n",
    "    # print('\\nCluster Sizes:')\n",
    "    # for cluster, size in zip(unique, counts):\n",
    "    #     print(f'Cluster {cluster}: {size} songs ({size/len(labels)*100:.1f}%)')\n",
    "    \n",
    "    # analyze cluster characteristics\n",
    "    cluster_centers = pd.DataFrame(\n",
    "        centers,\n",
    "        columns=feature_names\n",
    "    )\n",
    "    \n",
    "    # print('\\nDominant Genres per Cluster:')\n",
    "    # for i in range(len(centers)):\n",
    "    #     top_genres = cluster_centers.iloc[i].nlargest(3)\n",
    "    #     print(f\"Cluster {i}: {', '.join([f'{g} ({v:.2f})' for g, v in top_genres.items()])}\")\n",
    "    \n",
    "    # visualizations\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # PCA visualization of clusters\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_features = pca.fit_transform(scaled_features)\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    scatter = plt.scatter(reduced_features[:, 0], reduced_features[:, 1], \n",
    "                         c=labels, cmap='viridis', alpha=0.6)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title('Cluster Visualization (PCA)')\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Component')\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.bar(unique, counts)\n",
    "    plt.title('Cluster Size Distribution')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Number of Songs')\n",
    "    \n",
    "    plt.figure(figsize=(14, 20))\n",
    "    plt.subplot(2, 2, (3, 4))\n",
    "    sns.heatmap(cluster_centers, annot=True, cmap='Greens', fmt='.2f')\n",
    "    plt.title('Cluster Centers Heatmap')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Cluster')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return silhouette, inertia\n",
    "\n",
    "best_k = 59\n",
    "best_model = KMeans(n_clusters=best_k, random_state=42)\n",
    "best_model.fit(scaled_features)\n",
    "\n",
    "# evaluate the model\n",
    "silhouette, inertia = evaluate_clustering(\n",
    "    scaled_features, \n",
    "    best_model, \n",
    "    features.columns\n",
    ")\n",
    "\n",
    "# save best model\n",
    "with open('../models/kmeans_genre_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select best performer (k-means) and label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over different values of k (number of clusters)\n",
    "k = result['k']  # number of clusters from `result`\n",
    "\n",
    "# Create the KMeans model with the selected number of clusters\n",
    "model = KMeans(n_clusters=k, random_state=42)\n",
    "\n",
    "# Fit the model to the scaled features\n",
    "model.fit(scaled_features)\n",
    "\n",
    "# Store the model in a dictionary, keyed by the number of clusters\n",
    "final_models[k] = model\n",
    "\n",
    "# Optionally, you can add the cluster labels to the DataFrame\n",
    "df['cluster'] = model.labels_\n",
    "\n",
    "# Save the dataset with the cluster labels\n",
    "# df.to_csv('7_clustered_dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
